{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40596, 204)\n",
      "(13533, 204)\n",
      "(40596, 1)\n",
      "(13533, 1)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torch\n",
    "from data_process import load_mat\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNet(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_output):\n",
    "        super(FullyConnectedNet, self).__init__()\n",
    "        self.hidden_size_1 = 512\n",
    "        self.hidden_size_2 = 320\n",
    "        self.hidden_size_3 = 128\n",
    "\n",
    "        self.hidden_1 = nn.Linear(n_feature, self.hidden_size_1)\n",
    "        self.hidden_2 = nn.Linear(self.hidden_size_1, self.hidden_size_2)\n",
    "        self.hidden_3 = nn.Linear(self.hidden_size_2, self.hidden_size_3)\n",
    "        self.out = nn.Linear(self.hidden_size_3, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_1(x))\n",
    "        x = F.relu(self.hidden_2(x))\n",
    "        x = F.relu(self.hidden_3(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = './data2/Salinas_corrected.mat'\n",
    "label_path = './data2/Salinas_gt.mat'\n",
    "train_x, test_x, train_y, test_y = load_mat(data_path, label_path, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = Variable(torch.FloatTensor(train_x)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = Variable(torch.FloatTensor(test_x)).cuda()\n",
    "train_y = Variable(torch.LongTensor(np.squeeze(np.asarray(train_y, dtype=np.int64)))).cuda()\n",
    "test_y = Variable(torch.LongTensor(np.squeeze(np.asarray(test_y, dtype=np.int64))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fcnn = FullyConnectedNet(n_feature=204, n_output=16)\n",
    "\n",
    "fcnn.cuda()\n",
    "\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 200\n",
    "LR = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(fcnn.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss: 2.770817279815674\n",
      "epoch 1, train loss: 2.664440393447876\n",
      "epoch 2, train loss: 2.553398609161377\n",
      "epoch 3, train loss: 2.41252064704895\n",
      "epoch 4, train loss: 2.2373299598693848\n",
      "epoch 5, train loss: 2.038421630859375\n",
      "epoch 6, train loss: 1.8411697149276733\n",
      "epoch 7, train loss: 1.6762698888778687\n",
      "epoch 8, train loss: 1.5450658798217773\n",
      "epoch 9, train loss: 1.42690110206604\n",
      "epoch 10, train loss: 1.324112892150879\n",
      "epoch 11, train loss: 1.2323700189590454\n",
      "epoch 12, train loss: 1.1575027704238892\n",
      "epoch 13, train loss: 1.099145770072937\n",
      "epoch 14, train loss: 1.047837257385254\n",
      "epoch 15, train loss: 0.9942001104354858\n",
      "epoch 16, train loss: 0.964367151260376\n",
      "epoch 17, train loss: 0.9293386936187744\n",
      "epoch 18, train loss: 0.8928641676902771\n",
      "epoch 19, train loss: 0.8692101836204529\n",
      "epoch 20, train loss: 0.8375950455665588\n",
      "epoch 21, train loss: 0.8113358020782471\n",
      "epoch 22, train loss: 0.7872058153152466\n",
      "epoch 23, train loss: 0.7648479342460632\n",
      "epoch 24, train loss: 0.7473110556602478\n",
      "epoch 25, train loss: 0.7230852842330933\n",
      "epoch 26, train loss: 0.702613115310669\n",
      "epoch 27, train loss: 0.6906777620315552\n",
      "epoch 28, train loss: 0.6766059994697571\n",
      "epoch 29, train loss: 0.6646353602409363\n",
      "epoch 30, train loss: 0.6535532474517822\n",
      "epoch 31, train loss: 0.6380000710487366\n",
      "epoch 32, train loss: 0.6279037594795227\n",
      "epoch 33, train loss: 0.6195755004882812\n",
      "epoch 34, train loss: 0.6061826944351196\n",
      "epoch 35, train loss: 0.5984845757484436\n",
      "epoch 36, train loss: 0.5932859778404236\n",
      "epoch 37, train loss: 0.5835334658622742\n",
      "epoch 38, train loss: 0.5769143104553223\n",
      "epoch 39, train loss: 0.5715687870979309\n",
      "epoch 40, train loss: 0.5671848058700562\n",
      "epoch 41, train loss: 0.5610133409500122\n",
      "epoch 42, train loss: 0.5515085458755493\n",
      "epoch 43, train loss: 0.538463294506073\n",
      "epoch 44, train loss: 0.5361796021461487\n",
      "epoch 45, train loss: 0.532279372215271\n",
      "epoch 46, train loss: 0.518669843673706\n",
      "epoch 47, train loss: 0.5164525508880615\n",
      "epoch 48, train loss: 0.5129843354225159\n",
      "epoch 49, train loss: 0.49962401390075684\n",
      "epoch 50, train loss: 0.4981600344181061\n",
      "epoch 51, train loss: 0.4932500720024109\n",
      "epoch 52, train loss: 0.4816950559616089\n",
      "epoch 53, train loss: 0.4823745787143707\n",
      "epoch 54, train loss: 0.47607100009918213\n",
      "epoch 55, train loss: 0.46651536226272583\n",
      "epoch 56, train loss: 0.467340350151062\n",
      "epoch 57, train loss: 0.4594901204109192\n",
      "epoch 58, train loss: 0.45190533995628357\n",
      "epoch 59, train loss: 0.4516935348510742\n",
      "epoch 60, train loss: 0.44335293769836426\n",
      "epoch 61, train loss: 0.4380200207233429\n",
      "epoch 62, train loss: 0.4363687038421631\n",
      "epoch 63, train loss: 0.42864790558815\n",
      "epoch 64, train loss: 0.4249289631843567\n",
      "epoch 65, train loss: 0.42201414704322815\n",
      "epoch 66, train loss: 0.4152287244796753\n",
      "epoch 67, train loss: 0.41179540753364563\n",
      "epoch 68, train loss: 0.4080754816532135\n",
      "epoch 69, train loss: 0.4019085168838501\n",
      "epoch 70, train loss: 0.39830905199050903\n",
      "epoch 71, train loss: 0.39459943771362305\n",
      "epoch 72, train loss: 0.3889312744140625\n",
      "epoch 73, train loss: 0.38502374291419983\n",
      "epoch 74, train loss: 0.3814506232738495\n",
      "epoch 75, train loss: 0.37631693482398987\n",
      "epoch 76, train loss: 0.3723088502883911\n",
      "epoch 77, train loss: 0.36879202723503113\n",
      "epoch 78, train loss: 0.3642285466194153\n",
      "epoch 79, train loss: 0.3604333996772766\n",
      "epoch 80, train loss: 0.35690733790397644\n",
      "epoch 81, train loss: 0.352613627910614\n",
      "epoch 82, train loss: 0.3489104211330414\n",
      "epoch 83, train loss: 0.3454922139644623\n",
      "epoch 84, train loss: 0.34162232279777527\n",
      "epoch 85, train loss: 0.338288277387619\n",
      "epoch 86, train loss: 0.3352046608924866\n",
      "epoch 87, train loss: 0.3324887752532959\n",
      "epoch 88, train loss: 0.33252277970314026\n",
      "epoch 89, train loss: 0.335615336894989\n",
      "epoch 90, train loss: 0.34150001406669617\n",
      "epoch 91, train loss: 0.32735633850097656\n",
      "epoch 92, train loss: 0.3183167278766632\n",
      "epoch 93, train loss: 0.32225123047828674\n",
      "epoch 94, train loss: 0.31983456015586853\n",
      "epoch 95, train loss: 0.3114669919013977\n",
      "epoch 96, train loss: 0.3104785978794098\n",
      "epoch 97, train loss: 0.31134459376335144\n",
      "epoch 98, train loss: 0.30661725997924805\n",
      "epoch 99, train loss: 0.302468478679657\n",
      "epoch 100, train loss: 0.30325305461883545\n",
      "epoch 101, train loss: 0.30222630500793457\n",
      "epoch 102, train loss: 0.296525239944458\n",
      "epoch 103, train loss: 0.29608026146888733\n",
      "epoch 104, train loss: 0.29646798968315125\n",
      "epoch 105, train loss: 0.29210028052330017\n",
      "epoch 106, train loss: 0.28962600231170654\n",
      "epoch 107, train loss: 0.2900872826576233\n",
      "epoch 108, train loss: 0.28840717673301697\n",
      "epoch 109, train loss: 0.2851641774177551\n",
      "epoch 110, train loss: 0.283580482006073\n",
      "epoch 111, train loss: 0.2830859124660492\n",
      "epoch 112, train loss: 0.2823728322982788\n",
      "epoch 113, train loss: 0.27967366576194763\n",
      "epoch 114, train loss: 0.27737662196159363\n",
      "epoch 115, train loss: 0.2763579189777374\n",
      "epoch 116, train loss: 0.2758848965167999\n",
      "epoch 117, train loss: 0.27501043677330017\n",
      "epoch 118, train loss: 0.2731505036354065\n",
      "epoch 119, train loss: 0.271222323179245\n",
      "epoch 120, train loss: 0.2693405747413635\n",
      "epoch 121, train loss: 0.2680629789829254\n",
      "epoch 122, train loss: 0.2671501636505127\n",
      "epoch 123, train loss: 0.2665145695209503\n",
      "epoch 124, train loss: 0.26632675528526306\n",
      "epoch 125, train loss: 0.2664788067340851\n",
      "epoch 126, train loss: 0.26800987124443054\n",
      "epoch 127, train loss: 0.2681010365486145\n",
      "epoch 128, train loss: 0.2688360810279846\n",
      "epoch 129, train loss: 0.26378628611564636\n",
      "epoch 130, train loss: 0.25934839248657227\n",
      "epoch 131, train loss: 0.25717893242836\n",
      "epoch 132, train loss: 0.2580491006374359\n",
      "epoch 133, train loss: 0.2600696384906769\n",
      "epoch 134, train loss: 0.2591089606285095\n",
      "epoch 135, train loss: 0.25702494382858276\n",
      "epoch 136, train loss: 0.2534273564815521\n",
      "epoch 137, train loss: 0.2516991198062897\n",
      "epoch 138, train loss: 0.2520003616809845\n",
      "epoch 139, train loss: 0.2525436580181122\n",
      "epoch 140, train loss: 0.2523839473724365\n",
      "epoch 141, train loss: 0.2500346601009369\n",
      "epoch 142, train loss: 0.24783076345920563\n",
      "epoch 143, train loss: 0.2465200424194336\n",
      "epoch 144, train loss: 0.2462712824344635\n",
      "epoch 145, train loss: 0.2466212660074234\n",
      "epoch 146, train loss: 0.2466142624616623\n",
      "epoch 147, train loss: 0.24653226137161255\n",
      "epoch 148, train loss: 0.24506095051765442\n",
      "epoch 149, train loss: 0.24347805976867676\n",
      "epoch 150, train loss: 0.2415371835231781\n",
      "epoch 151, train loss: 0.24015364050865173\n",
      "epoch 152, train loss: 0.23939372599124908\n",
      "epoch 153, train loss: 0.23916122317314148\n",
      "epoch 154, train loss: 0.23950892686843872\n",
      "epoch 155, train loss: 0.240367591381073\n",
      "epoch 156, train loss: 0.24281717836856842\n",
      "epoch 157, train loss: 0.24371010065078735\n",
      "epoch 158, train loss: 0.24490559101104736\n",
      "epoch 159, train loss: 0.23897051811218262\n",
      "epoch 160, train loss: 0.2343696653842926\n",
      "epoch 161, train loss: 0.23344357311725616\n",
      "epoch 162, train loss: 0.23549380898475647\n",
      "epoch 163, train loss: 0.23766712844371796\n",
      "epoch 164, train loss: 0.23501968383789062\n",
      "epoch 165, train loss: 0.23176901042461395\n",
      "epoch 166, train loss: 0.22956693172454834\n",
      "epoch 167, train loss: 0.2298586219549179\n",
      "epoch 168, train loss: 0.2311929166316986\n",
      "epoch 169, train loss: 0.23053757846355438\n",
      "epoch 170, train loss: 0.22874696552753448\n",
      "epoch 171, train loss: 0.22653762996196747\n",
      "epoch 172, train loss: 0.22570036351680756\n",
      "epoch 173, train loss: 0.2260667383670807\n",
      "epoch 174, train loss: 0.22640646994113922\n",
      "epoch 175, train loss: 0.22638241946697235\n",
      "epoch 176, train loss: 0.22499267756938934\n",
      "epoch 177, train loss: 0.22347144782543182\n",
      "epoch 178, train loss: 0.22200171649456024\n",
      "epoch 179, train loss: 0.22108176350593567\n",
      "epoch 180, train loss: 0.22068260610103607\n",
      "epoch 181, train loss: 0.2206156849861145\n",
      "epoch 182, train loss: 0.22085274755954742\n",
      "epoch 183, train loss: 0.22101503610610962\n",
      "epoch 184, train loss: 0.22180821001529694\n",
      "epoch 185, train loss: 0.22172945737838745\n",
      "epoch 186, train loss: 0.22207069396972656\n",
      "epoch 187, train loss: 0.2200467437505722\n",
      "epoch 188, train loss: 0.21800856292247772\n",
      "epoch 189, train loss: 0.21564500033855438\n",
      "epoch 190, train loss: 0.21440309286117554\n",
      "epoch 191, train loss: 0.21426545083522797\n",
      "epoch 192, train loss: 0.21487028896808624\n",
      "epoch 193, train loss: 0.21632930636405945\n",
      "epoch 194, train loss: 0.2175714373588562\n",
      "epoch 195, train loss: 0.22013802826404572\n",
      "epoch 196, train loss: 0.21860669553279877\n",
      "epoch 197, train loss: 0.21647535264492035\n",
      "epoch 198, train loss: 0.21161063015460968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199, train loss: 0.20941980183124542\n",
      "epoch 200, train loss: 0.21034808456897736\n",
      "epoch 201, train loss: 0.21212828159332275\n",
      "epoch 202, train loss: 0.21368472278118134\n",
      "epoch 203, train loss: 0.21165160834789276\n",
      "epoch 204, train loss: 0.2096174657344818\n",
      "epoch 205, train loss: 0.20758147537708282\n",
      "epoch 206, train loss: 0.2065451294183731\n",
      "epoch 207, train loss: 0.2063063383102417\n",
      "epoch 208, train loss: 0.20573829114437103\n",
      "epoch 209, train loss: 0.20616267621517181\n",
      "epoch 210, train loss: 0.20715080201625824\n",
      "epoch 211, train loss: 0.2083965539932251\n",
      "epoch 212, train loss: 0.20834706723690033\n",
      "epoch 213, train loss: 0.2081274837255478\n",
      "epoch 214, train loss: 0.20586179196834564\n",
      "epoch 215, train loss: 0.20395039021968842\n",
      "epoch 216, train loss: 0.20130416750907898\n",
      "epoch 217, train loss: 0.2001170665025711\n",
      "epoch 218, train loss: 0.20067468285560608\n",
      "epoch 219, train loss: 0.20186658203601837\n",
      "epoch 220, train loss: 0.20349161326885223\n",
      "epoch 221, train loss: 0.20358102023601532\n",
      "epoch 222, train loss: 0.20444069802761078\n",
      "epoch 223, train loss: 0.20235781371593475\n",
      "epoch 224, train loss: 0.1999824494123459\n",
      "epoch 225, train loss: 0.19711293280124664\n",
      "epoch 226, train loss: 0.19578301906585693\n",
      "epoch 227, train loss: 0.19611197710037231\n",
      "epoch 228, train loss: 0.19722336530685425\n",
      "epoch 229, train loss: 0.19867458939552307\n",
      "epoch 230, train loss: 0.1990390419960022\n",
      "epoch 231, train loss: 0.20001117885112762\n",
      "epoch 232, train loss: 0.19809497892856598\n",
      "epoch 233, train loss: 0.19621151685714722\n",
      "epoch 234, train loss: 0.19334755837917328\n",
      "epoch 235, train loss: 0.19181525707244873\n",
      "epoch 236, train loss: 0.1916850358247757\n",
      "epoch 237, train loss: 0.19240735471248627\n",
      "epoch 238, train loss: 0.19379517436027527\n",
      "epoch 239, train loss: 0.1947360634803772\n",
      "epoch 240, train loss: 0.196599543094635\n",
      "epoch 241, train loss: 0.19558954238891602\n",
      "epoch 242, train loss: 0.19461949169635773\n",
      "epoch 243, train loss: 0.19116538763046265\n",
      "epoch 244, train loss: 0.1887519210577011\n",
      "epoch 245, train loss: 0.18769904971122742\n",
      "epoch 246, train loss: 0.18803778290748596\n",
      "epoch 247, train loss: 0.18927760422229767\n",
      "epoch 248, train loss: 0.19034631550312042\n",
      "epoch 249, train loss: 0.19204077124595642\n",
      "epoch 250, train loss: 0.1913427859544754\n",
      "epoch 251, train loss: 0.19095303118228912\n",
      "epoch 252, train loss: 0.18814602494239807\n",
      "epoch 253, train loss: 0.1860312521457672\n",
      "epoch 254, train loss: 0.184465229511261\n",
      "epoch 255, train loss: 0.18394887447357178\n",
      "epoch 256, train loss: 0.18423347175121307\n",
      "epoch 257, train loss: 0.18494939804077148\n",
      "epoch 258, train loss: 0.18630637228488922\n",
      "epoch 259, train loss: 0.18710674345493317\n",
      "epoch 260, train loss: 0.18891234695911407\n",
      "epoch 261, train loss: 0.18830841779708862\n",
      "epoch 262, train loss: 0.18841969966888428\n",
      "epoch 263, train loss: 0.18536868691444397\n",
      "epoch 264, train loss: 0.18295247852802277\n",
      "epoch 265, train loss: 0.18084335327148438\n",
      "epoch 266, train loss: 0.18016314506530762\n",
      "epoch 267, train loss: 0.18068666756153107\n",
      "epoch 268, train loss: 0.18174824118614197\n",
      "epoch 269, train loss: 0.18341457843780518\n",
      "epoch 270, train loss: 0.18404251337051392\n",
      "epoch 271, train loss: 0.18541088700294495\n",
      "epoch 272, train loss: 0.18383349478244781\n",
      "epoch 273, train loss: 0.18260054290294647\n",
      "epoch 274, train loss: 0.17974600195884705\n",
      "epoch 275, train loss: 0.1778300404548645\n",
      "epoch 276, train loss: 0.17697742581367493\n",
      "epoch 277, train loss: 0.17718493938446045\n",
      "epoch 278, train loss: 0.17815560102462769\n",
      "epoch 279, train loss: 0.1792370229959488\n",
      "epoch 280, train loss: 0.18102777004241943\n",
      "epoch 281, train loss: 0.1813526153564453\n",
      "epoch 282, train loss: 0.18251971900463104\n",
      "epoch 283, train loss: 0.18047106266021729\n",
      "epoch 284, train loss: 0.17892737686634064\n",
      "epoch 285, train loss: 0.17608971893787384\n",
      "epoch 286, train loss: 0.17436346411705017\n",
      "epoch 287, train loss: 0.17381535470485687\n",
      "epoch 288, train loss: 0.174297034740448\n",
      "epoch 289, train loss: 0.1754903942346573\n",
      "epoch 290, train loss: 0.17655505239963531\n",
      "epoch 291, train loss: 0.17837806046009064\n",
      "epoch 292, train loss: 0.17834395170211792\n",
      "epoch 293, train loss: 0.17898046970367432\n",
      "epoch 294, train loss: 0.17660407721996307\n",
      "epoch 295, train loss: 0.17473304271697998\n",
      "epoch 296, train loss: 0.1724078506231308\n",
      "epoch 297, train loss: 0.17115674912929535\n",
      "epoch 298, train loss: 0.170907124876976\n",
      "epoch 299, train loss: 0.17143172025680542\n",
      "epoch 300, train loss: 0.17257831990718842\n",
      "epoch 301, train loss: 0.17372548580169678\n",
      "epoch 302, train loss: 0.17586207389831543\n",
      "epoch 303, train loss: 0.1763833910226822\n",
      "epoch 304, train loss: 0.17801064252853394\n",
      "epoch 305, train loss: 0.17553061246871948\n",
      "epoch 306, train loss: 0.1734643131494522\n",
      "epoch 307, train loss: 0.17007379233837128\n",
      "epoch 308, train loss: 0.1683996021747589\n",
      "epoch 309, train loss: 0.16854511201381683\n",
      "epoch 310, train loss: 0.16984020173549652\n",
      "epoch 311, train loss: 0.17182917892932892\n",
      "epoch 312, train loss: 0.172504723072052\n",
      "epoch 313, train loss: 0.17333093285560608\n",
      "epoch 314, train loss: 0.1712532639503479\n",
      "epoch 315, train loss: 0.16943275928497314\n",
      "epoch 316, train loss: 0.16750837862491608\n",
      "epoch 317, train loss: 0.16660568118095398\n",
      "epoch 318, train loss: 0.16697996854782104\n",
      "epoch 319, train loss: 0.16746897995471954\n",
      "epoch 320, train loss: 0.16846998035907745\n",
      "epoch 321, train loss: 0.1686725914478302\n",
      "epoch 322, train loss: 0.16998888552188873\n",
      "epoch 323, train loss: 0.17063622176647186\n",
      "epoch 324, train loss: 0.17218422889709473\n",
      "epoch 325, train loss: 0.17139044404029846\n",
      "epoch 326, train loss: 0.16929642856121063\n",
      "epoch 327, train loss: 0.16610479354858398\n",
      "epoch 328, train loss: 0.16481861472129822\n",
      "epoch 329, train loss: 0.16454672813415527\n",
      "epoch 330, train loss: 0.16561749577522278\n",
      "epoch 331, train loss: 0.16546030342578888\n",
      "epoch 332, train loss: 0.16625505685806274\n",
      "epoch 333, train loss: 0.16902785003185272\n",
      "epoch 334, train loss: 0.16922111809253693\n",
      "epoch 335, train loss: 0.16977109014987946\n",
      "epoch 336, train loss: 0.16731277108192444\n",
      "epoch 337, train loss: 0.16671079397201538\n",
      "epoch 338, train loss: 0.16621863842010498\n",
      "epoch 339, train loss: 0.1633860021829605\n",
      "epoch 340, train loss: 0.16186363995075226\n",
      "epoch 341, train loss: 0.16212689876556396\n",
      "epoch 342, train loss: 0.16255627572536469\n",
      "epoch 343, train loss: 0.16339635848999023\n",
      "epoch 344, train loss: 0.1648242324590683\n",
      "epoch 345, train loss: 0.1662006378173828\n",
      "epoch 346, train loss: 0.16856376826763153\n",
      "epoch 347, train loss: 0.16794364154338837\n",
      "epoch 348, train loss: 0.16810721158981323\n",
      "epoch 349, train loss: 0.1642603874206543\n",
      "epoch 350, train loss: 0.16136695444583893\n",
      "epoch 351, train loss: 0.15920154750347137\n",
      "epoch 352, train loss: 0.15950115025043488\n",
      "epoch 353, train loss: 0.16129358112812042\n",
      "epoch 354, train loss: 0.16270461678504944\n",
      "epoch 355, train loss: 0.16457439959049225\n",
      "epoch 356, train loss: 0.16352272033691406\n",
      "epoch 357, train loss: 0.1622144728899002\n",
      "epoch 358, train loss: 0.15949569642543793\n",
      "epoch 359, train loss: 0.15773670375347137\n",
      "epoch 360, train loss: 0.15738128125667572\n",
      "epoch 361, train loss: 0.15793433785438538\n",
      "epoch 362, train loss: 0.1592164784669876\n",
      "epoch 363, train loss: 0.1605921983718872\n",
      "epoch 364, train loss: 0.16260118782520294\n",
      "epoch 365, train loss: 0.1627074033021927\n",
      "epoch 366, train loss: 0.16295255720615387\n",
      "epoch 367, train loss: 0.16025355458259583\n",
      "epoch 368, train loss: 0.15806353092193604\n",
      "epoch 369, train loss: 0.15610849857330322\n",
      "epoch 370, train loss: 0.15549612045288086\n",
      "epoch 371, train loss: 0.15591853857040405\n",
      "epoch 372, train loss: 0.15695446729660034\n",
      "epoch 373, train loss: 0.1586567759513855\n",
      "epoch 374, train loss: 0.15959015488624573\n",
      "epoch 375, train loss: 0.1610872596502304\n",
      "epoch 376, train loss: 0.1602669656276703\n",
      "epoch 377, train loss: 0.15974658727645874\n",
      "epoch 378, train loss: 0.1572590172290802\n",
      "epoch 379, train loss: 0.1554047018289566\n",
      "epoch 380, train loss: 0.15400919318199158\n",
      "epoch 381, train loss: 0.1534818559885025\n",
      "epoch 382, train loss: 0.1536247581243515\n",
      "epoch 383, train loss: 0.15424121916294098\n",
      "epoch 384, train loss: 0.15534856915473938\n",
      "epoch 385, train loss: 0.15642212331295013\n",
      "epoch 386, train loss: 0.1584952026605606\n",
      "epoch 387, train loss: 0.15954124927520752\n",
      "epoch 388, train loss: 0.16204041242599487\n",
      "epoch 389, train loss: 0.16020940244197845\n",
      "epoch 390, train loss: 0.1584823876619339\n",
      "epoch 391, train loss: 0.15410484373569489\n",
      "epoch 392, train loss: 0.15189754962921143\n",
      "epoch 393, train loss: 0.15215565264225006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 394, train loss: 0.1540139764547348\n",
      "epoch 395, train loss: 0.1568821519613266\n",
      "epoch 396, train loss: 0.15777020156383514\n",
      "epoch 397, train loss: 0.1579631119966507\n",
      "epoch 398, train loss: 0.1542108952999115\n",
      "epoch 399, train loss: 0.15150469541549683\n",
      "epoch 400, train loss: 0.15056149661540985\n",
      "epoch 401, train loss: 0.15122713148593903\n",
      "epoch 402, train loss: 0.15317261219024658\n",
      "epoch 403, train loss: 0.15515421330928802\n",
      "epoch 404, train loss: 0.15734291076660156\n",
      "epoch 405, train loss: 0.1554403305053711\n",
      "epoch 406, train loss: 0.15320438146591187\n",
      "epoch 407, train loss: 0.15061292052268982\n",
      "epoch 408, train loss: 0.14924205839633942\n",
      "epoch 409, train loss: 0.14921852946281433\n",
      "epoch 410, train loss: 0.1503332406282425\n",
      "epoch 411, train loss: 0.15223905444145203\n",
      "epoch 412, train loss: 0.15368075668811798\n",
      "epoch 413, train loss: 0.15509817004203796\n",
      "epoch 414, train loss: 0.15413662791252136\n",
      "epoch 415, train loss: 0.1529257595539093\n",
      "epoch 416, train loss: 0.15037982165813446\n",
      "epoch 417, train loss: 0.14889781177043915\n",
      "epoch 418, train loss: 0.1482962965965271\n",
      "epoch 419, train loss: 0.14876684546470642\n",
      "epoch 420, train loss: 0.14897924661636353\n",
      "epoch 421, train loss: 0.15003879368305206\n",
      "epoch 422, train loss: 0.1527099311351776\n",
      "epoch 423, train loss: 0.15439891815185547\n",
      "epoch 424, train loss: 0.15663239359855652\n",
      "epoch 425, train loss: 0.1535869836807251\n",
      "epoch 426, train loss: 0.15016233921051025\n",
      "epoch 427, train loss: 0.1477244794368744\n",
      "epoch 428, train loss: 0.14755798876285553\n",
      "epoch 429, train loss: 0.14961399137973785\n",
      "epoch 430, train loss: 0.15133115649223328\n",
      "epoch 431, train loss: 0.15164880454540253\n",
      "epoch 432, train loss: 0.1503380686044693\n",
      "epoch 433, train loss: 0.1546545773744583\n",
      "epoch 434, train loss: 0.17133940756320953\n",
      "epoch 435, train loss: 0.22031041979789734\n",
      "epoch 436, train loss: 0.370238721370697\n",
      "epoch 437, train loss: 0.20195657014846802\n",
      "epoch 438, train loss: 0.3211154043674469\n",
      "epoch 439, train loss: 0.25346848368644714\n",
      "epoch 440, train loss: 0.42131757736206055\n",
      "epoch 441, train loss: 0.25688356161117554\n",
      "epoch 442, train loss: 0.30208420753479004\n",
      "epoch 443, train loss: 0.32733574509620667\n",
      "epoch 444, train loss: 0.9654448628425598\n",
      "epoch 445, train loss: 0.27052041888237\n",
      "epoch 446, train loss: 0.8784224390983582\n",
      "epoch 447, train loss: 0.2990929186344147\n",
      "epoch 448, train loss: 0.4631287157535553\n",
      "epoch 449, train loss: 0.5771418213844299\n",
      "epoch 450, train loss: 0.32463327050209045\n",
      "epoch 451, train loss: 0.26396089792251587\n",
      "epoch 452, train loss: 0.35010021924972534\n",
      "epoch 453, train loss: 0.3555988073348999\n",
      "epoch 454, train loss: 0.2268308848142624\n",
      "epoch 455, train loss: 0.24105647206306458\n",
      "epoch 456, train loss: 0.24867302179336548\n",
      "epoch 457, train loss: 0.27769121527671814\n",
      "epoch 458, train loss: 0.2316032350063324\n",
      "epoch 459, train loss: 0.2144768089056015\n",
      "epoch 460, train loss: 0.19433268904685974\n",
      "epoch 461, train loss: 0.22992345690727234\n",
      "epoch 462, train loss: 0.22321036458015442\n",
      "epoch 463, train loss: 0.2082628756761551\n",
      "epoch 464, train loss: 0.18621249496936798\n",
      "epoch 465, train loss: 0.18667247891426086\n",
      "epoch 466, train loss: 0.19184610247612\n",
      "epoch 467, train loss: 0.19324228167533875\n",
      "epoch 468, train loss: 0.19457703828811646\n",
      "epoch 469, train loss: 0.18160973489284515\n",
      "epoch 470, train loss: 0.18361414968967438\n",
      "epoch 471, train loss: 0.17690986394882202\n",
      "epoch 472, train loss: 0.17935217916965485\n",
      "epoch 473, train loss: 0.17591024935245514\n",
      "epoch 474, train loss: 0.17642982304096222\n",
      "epoch 475, train loss: 0.17555145919322968\n",
      "epoch 476, train loss: 0.17239108681678772\n",
      "epoch 477, train loss: 0.1716613620519638\n",
      "epoch 478, train loss: 0.1677965223789215\n",
      "epoch 479, train loss: 0.16869524121284485\n",
      "epoch 480, train loss: 0.1659337282180786\n",
      "epoch 481, train loss: 0.16720633208751678\n",
      "epoch 482, train loss: 0.16472257673740387\n",
      "epoch 483, train loss: 0.1654253900051117\n",
      "epoch 484, train loss: 0.16311247646808624\n",
      "epoch 485, train loss: 0.16309726238250732\n",
      "epoch 486, train loss: 0.161348357796669\n",
      "epoch 487, train loss: 0.16121365129947662\n",
      "epoch 488, train loss: 0.1602231115102768\n",
      "epoch 489, train loss: 0.16011065244674683\n",
      "epoch 490, train loss: 0.1594788283109665\n",
      "epoch 491, train loss: 0.15923762321472168\n",
      "epoch 492, train loss: 0.15825311839580536\n",
      "epoch 493, train loss: 0.15779878199100494\n",
      "epoch 494, train loss: 0.15706585347652435\n",
      "epoch 495, train loss: 0.15707233548164368\n",
      "epoch 496, train loss: 0.1565702110528946\n",
      "epoch 497, train loss: 0.15632620453834534\n",
      "epoch 498, train loss: 0.15549622476100922\n",
      "epoch 499, train loss: 0.1551549732685089\n"
     ]
    }
   ],
   "source": [
    "for t in range(EPOCH):\n",
    "    prediction = fcnn(train_x)\n",
    "    loss = loss_func(prediction, train_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('epoch {}, train loss: {}'.format(t, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = torch.max(fcnn(test_x), 1)[1].data.numpy().squeeze()\n",
    "print(accuracy_score(test_pred, test_y.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
